{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15abb1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2b9270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"You OpenAI Key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "355e156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfreader=PdfReader('DeepLearning paper.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e172bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read text from PDF\n",
    "raw_text = \"\"\n",
    "for i, page in enumerate(pdfreader.pages):\n",
    "    content = page.extract_text()\n",
    "    if content:\n",
    "        raw_text += content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e0ff52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the text using Character Text Splitter\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    ")\n",
    "texts = text_splitter.split_text(raw_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "137f3284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download embeddings from OpenAI\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e761bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating FAISS vector store from texts and embeddings\n",
    "document_search = FAISS.from_texts(texts, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87fcf9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load QA chain with a OpenAI model (e.g., text-davinci-002)\n",
    "chain = load_qa_chain(OpenAI(model_name=\"gpt-3.5-turbo-instruct\"), chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7fa3e8",
   "metadata": {},
   "source": [
    "## Query1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ed57c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify your question\n",
    "query = \"Explain Positional Encoding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5cc1f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Similarity search\n",
    "docs = document_search.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6c1e97f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nPositional encoding is a technique used in natural language processing models, specifically in self-attention layers, to inject information about the relative or absolute position of tokens in a sequence. This is necessary because the models do not have any inherent understanding of the order of the sequence, since they do not use recurrent or convolutional layers. The positional encodings have the same dimension as the input embeddings and are added to them, allowing the model to learn to attend to relative positions. The encoding is calculated using sine and cosine functions with different frequencies, forming a geometric progression. This function allows the model to easily learn to attend by relative positions, as each dimension of the positional encoding corresponds to a sinusoid. The use of positional encoding helps improve the performance of the model in tasks that require understanding of sequential information. '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the QA chain\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd538dda",
   "metadata": {},
   "source": [
    "## Query2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "169cc909",
   "metadata": {},
   "outputs": [],
   "source": [
    "query2 = \"Who created transformers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc1ec445",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = document_search.similarity_search(query2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c2e93f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Jakob, Ashish, Illia, Noam, Niki, and Llion created the Transformer model.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(input_documents=docs, question=query2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89126bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
